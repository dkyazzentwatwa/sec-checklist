/**
 * useWebLLM Hook
 *
 * React hook for interacting with the local WebLLM engine via Web Worker.
 * Handles model loading, chat generation, and state management.
 * All AI processing runs in a separate thread for better UI responsiveness.
 */

import { useCallback, useState } from 'react'
import { useTranslation } from 'react-i18next'
import { workerEngine } from '../services/webllm/workerEngine'
import type { ModelId, ChatMessage, GenerateOptions } from '../services/webllm/engine'
import { getSystemPrompt, type AssistantType } from '../services/webllm/prompts'
import {
  useAIStore,
  selectIsModelReady,
  selectIsModelLoading,
  selectActiveConversation,
} from '../stores/aiStore'

interface UseWebLLMOptions {
  assistantType?: AssistantType
  context?: string
}

export function useWebLLM(options: UseWebLLMOptions = {}) {
  const { assistantType = 'general', context } = options
  const { i18n } = useTranslation()
  const lang = (i18n.language === 'es' ? 'es' : 'en') as 'en' | 'es'

  const [isGenerating, setIsGenerating] = useState(false)
  const [generationError, setGenerationError] = useState<string | null>(null)

  // Store state
  const isModelReady = useAIStore(selectIsModelReady)
  const isModelLoading = useAIStore(selectIsModelLoading)
  const activeConversation = useAIStore(selectActiveConversation)
  const modelProgress = useAIStore((s) => s.modelProgress)
  const modelError = useAIStore((s) => s.modelError)
  const preferredModel = useAIStore((s) => s.preferredModel)

  // Store actions
  const setModelStatus = useAIStore((s) => s.setModelStatus)
  const setModelProgress = useAIStore((s) => s.setModelProgress)
  const setCurrentModel = useAIStore((s) => s.setCurrentModel)
  const setModelError = useAIStore((s) => s.setModelError)
  const createConversation = useAIStore((s) => s.createConversation)
  const addMessage = useAIStore((s) => s.addMessage)
  const updateLastAssistantMessage = useAIStore((s) => s.updateLastAssistantMessage)

  /**
   * Check if WebGPU is available
   */
  const checkSupport = useCallback(async () => {
    const result = await workerEngine.checkWebGPUSupport()
    return result
  }, [])

  /**
   * Load the AI model (runs in Web Worker)
   */
  const loadModel = useCallback(
    async (modelId?: ModelId) => {
      const model = modelId || preferredModel

      setModelError(null)
      setModelStatus('downloading')

      try {
        await workerEngine.loadModel(model, (progress) => {
          setModelProgress(progress)

          if (progress.stage === 'downloading') {
            setModelStatus('downloading')
          } else if (progress.stage === 'loading') {
            setModelStatus('loading')
          } else if (progress.stage === 'ready') {
            setModelStatus('ready')
            setCurrentModel(model)
          } else if (progress.stage === 'error') {
            setModelStatus('error')
            setModelError(progress.text)
          }
        })
      } catch (error) {
        setModelStatus('error')
        setModelError(error instanceof Error ? error.message : 'Failed to load model')
        throw error
      }
    },
    [preferredModel, setModelError, setModelStatus, setModelProgress, setCurrentModel]
  )

  /**
   * Send a message and get a response (runs in Web Worker)
   */
  const sendMessage = useCallback(
    async (
      content: string,
      options: { stream?: boolean; conversationId?: string } = {}
    ) => {
      const { stream = true, conversationId } = options

      if (!isModelReady) {
        throw new Error('Model not loaded')
      }

      setIsGenerating(true)
      setGenerationError(null)

      // Get or create conversation
      let convId = conversationId || activeConversation?.id
      if (!convId) {
        convId = createConversation(assistantType)
      }

      // Get conversation messages
      const conversation = useAIStore.getState().conversations.find((c) => c.id === convId)
      const existingMessages = conversation?.messages || []

      // Build messages array with system prompt
      const systemPrompt = getSystemPrompt({ type: assistantType, language: lang, context })
      const messages: ChatMessage[] = [
        { role: 'system', content: systemPrompt },
        ...existingMessages,
        { role: 'user', content },
      ]

      // Add user message to store
      addMessage(convId, { role: 'user', content })

      // Add placeholder for assistant response
      addMessage(convId, { role: 'assistant', content: '' })

      try {
        const generateOptions: GenerateOptions = {
          maxTokens: 1024,
          temperature: 0.7,
        }

        if (stream) {
          generateOptions.onToken = (token) => {
            // Update the assistant message as tokens arrive
            const conv = useAIStore.getState().conversations.find((c) => c.id === convId)
            const lastMsg = conv?.messages[conv.messages.length - 1]
            if (lastMsg && lastMsg.role === 'assistant') {
              updateLastAssistantMessage(convId!, lastMsg.content + token)
            }
          }
        }

        const response = await workerEngine.generate(messages, generateOptions)

        // If not streaming, update with full response
        if (!stream) {
          updateLastAssistantMessage(convId!, response)
        }

        return response
      } catch (error) {
        const errorMessage = error instanceof Error ? error.message : 'Generation failed'
        setGenerationError(errorMessage)

        // Update assistant message with error
        updateLastAssistantMessage(convId!, `Error: ${errorMessage}`)

        throw error
      } finally {
        setIsGenerating(false)
      }
    },
    [
      isModelReady,
      activeConversation,
      assistantType,
      lang,
      context,
      createConversation,
      addMessage,
      updateLastAssistantMessage,
    ]
  )

  /**
   * Reset the chat context
   */
  const resetChat = useCallback(async () => {
    await workerEngine.resetChat()
  }, [])

  /**
   * Unload the model to free memory
   */
  const unloadModel = useCallback(async () => {
    await workerEngine.unload()
    setModelStatus('idle')
    setCurrentModel(null)
    setModelProgress(null)
  }, [setModelStatus, setCurrentModel, setModelProgress])

  /**
   * Get performance stats
   */
  const getStats = useCallback(async () => {
    return workerEngine.getStats()
  }, [])

  return {
    // State
    isModelReady,
    isModelLoading,
    isGenerating,
    modelProgress,
    modelError,
    generationError,
    activeConversation,

    // Actions
    checkSupport,
    loadModel,
    sendMessage,
    resetChat,
    unloadModel,
    getStats,
  }
}
